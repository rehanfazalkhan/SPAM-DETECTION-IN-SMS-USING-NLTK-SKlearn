{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing for Text Classification with NLTK and Scikit-learn FOR SMS SPAM DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\SRKT\\Desktop\\SMSspam\",sep='\\t',header=None,encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check class distribution\n",
    "df[0].value_counts() # describe whic is spam or ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoder convert class level to binary values, 1=spam / 0=ham\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "y=encoder.fit_transform(df[0])\n",
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "5    FreeMsg Hey there darling it's been 3 week's n...\n",
       "6    Even my brother is not like to speak with me. ...\n",
       "7    As per your request 'Melle Melle (Oru Minnamin...\n",
       "8    WINNER!! As a valued network customer you have...\n",
       "9    Had your mobile 11 months or more? U R entitle...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store the sms msg data\n",
    "text_msg=df[1]\n",
    "text_msg[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Regular expression to replace email address,URL,phone no.,other no.\n",
    "\n",
    "#replace email address with email\n",
    "processed=text_msg.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','emailaddress')\n",
    "\n",
    "#replace URL with webaddress\n",
    "processed=processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','webaddress')\n",
    "\n",
    "#replace 10 digit phone no with phonenumber\n",
    "processed=processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','phonenumber')\n",
    "\n",
    "#replace money symbol with moneysymb\n",
    "processed=processed.str.replace(r'£|\\$', 'moneysymb')\n",
    "\n",
    "#replace numbers with numbrs\n",
    "processed=processed.str.replace(r'£|\\$', 'moneysymb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "processed=processed.str.replace(r'[^\\w\\d\\s]',' ')\n",
    "\n",
    "#replace whitespace betweens terms with single space\n",
    "processed=processed.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     go until jurong point crazy available only in ...\n",
       "1                               ok lar joking wif u oni\n",
       "2     free entry in 2 a wkly comp to win fa cup fina...\n",
       "3           u dun say so early hor u c already then say\n",
       "4     nah i don t think he goes to usf he lives arou...\n",
       "5     freemsg hey there darling it s been 3 week s n...\n",
       "6     even my brother is not like to speak with me t...\n",
       "7     as per your request melle melle oru minnaminun...\n",
       "8     winner as a valued network customer you have b...\n",
       "9     had your mobile 11 months or more u r entitled...\n",
       "10    i m gonna be home soon and i don t want to tal...\n",
       "11    six chances to win cash from 100 to 20 000 pou...\n",
       "12    urgent you have won a 1 week free membership i...\n",
       "13    i ve been searching for the right words to tha...\n",
       "14                    i have a date on sunday with will\n",
       "15    xxxmobilemovieclub to use your credit click th...\n",
       "16                               oh k i m watching here\n",
       "17    eh u remember how 2 spell his name yes i did h...\n",
       "18    fine if that s the way u feel that s the way i...\n",
       "19    england v macedonia dont miss the goals team n...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chnage words to lowercase\n",
    "processed=processed.str.lower()\n",
    "processed[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords from text\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "processed=processed.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\n",
    "\n",
    "#remove word stems using a Porter Stemmer\n",
    "ps=nltk.PorterStemmer()\n",
    "processed=processed.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go jurong point crazi avail bugi n great world...\n",
       "1                                ok lar joke wif u oni\n",
       "2    free entri 2 wkli comp win fa cup final tkt 21...\n",
       "3                  u dun say earli hor u c alreadi say\n",
       "4                 nah think goe usf live around though\n",
       "5    freemsg hey darl 3 week word back like fun sti...\n",
       "6        even brother like speak treat like aid patent\n",
       "7    per request mell mell oru minnaminungint nurun...\n",
       "8    winner valu network custom select receivea mon...\n",
       "9    mobil 11 month u r entitl updat latest colour ...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Bag of Words\n",
    "from nltk.tokenize import word_tokenize\n",
    "all_words=[]\n",
    "for message in processed:\n",
    "    words=word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "all_words=nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of words 7309\n",
      "most common words [('u', 1207), ('call', 674), ('2', 519), ('go', 456), ('get', 451), ('ur', 391), ('4', 323), ('gt', 318), ('lt', 316), ('come', 304), ('ok', 293), ('free', 284), ('day', 276), ('know', 275), ('love', 266)]\n"
     ]
    }
   ],
   "source": [
    "print(\"total no of words\",len(all_words))\n",
    "print(\"most common words\",all_words.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 1500 most common words\n",
    "word_feature=list(all_words.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "jurong\n",
      "point\n",
      "crazi\n",
      "avail\n",
      "bugi\n",
      "n\n",
      "great\n",
      "world\n",
      "la\n",
      "e\n",
      "buffet\n",
      "cine\n",
      "got\n",
      "amor\n",
      "wat\n"
     ]
    }
   ],
   "source": [
    "#the find_feature function will determine which of the 1500 word features are contained in the review\n",
    "def find_features(message):\n",
    "    words=word_tokenize(message)\n",
    "    feature={}\n",
    "    for word in word_feature:\n",
    "        feature[word]=(word in words)\n",
    "    return feature\n",
    "\n",
    "#example\n",
    "features=find_features(processed[0])\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets do for all message \n",
    "messages=zip(processed,y)\n",
    "\n",
    "featuresets = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# split the data into training and testing datasets\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179\n",
      "1393\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 98.49246231155779\n"
     ]
    }
   ],
   "source": [
    "# We can use sklearn algorithms in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# train the model on the training data\n",
    "model.train(training)\n",
    "\n",
    "# and test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn Classifiers with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can use sklearn algorithm in nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model=SklearnClassifier(SVC(kernel='linear'))\n",
    "model.train(training)\n",
    "\n",
    "#add test on testing datasets\n",
    "accuracy=nltk.classify.accuracy(model,testing)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.49246231155779"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
